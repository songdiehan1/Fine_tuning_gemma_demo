# Fine-tuning Gemma-3-4B with LoRA on Google Colab

This project fine-tunes the Gemma-3-4B model using LoRA (Low-Rank Adaptation), optimized with Unsloth + PEFT + SFT. The training is performed on Google Colab with 4-bit quantization (QLoRA) to reduce VRAM usage, using high-quality Chinese Zhihu dataset.

## ðŸ“Œ Key Features

âš¡ Optimized with Unsloth, making fine-tuning 2-4x faster than traditional methods.

ðŸŽ¯ Uses LoRA fine-tuning, reducing computational cost.

ðŸ“– Trains on high-quality Zhihu SFT dataset.

âœ… Compatible with Google Colab for easy execution.

## ðŸš€ Quick Start

ðŸ“Œ Click the link below to open the Colab notebook and start training immediately:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1O3WAp2oBoayjeMwB57RLoA5SVUcjcasM)

Alternatively, you can clone the repository and run it manually

'''
git clone https://github.com/yourusername/gemma-lora-colab.git
cd gemma-lora-colab
'''
